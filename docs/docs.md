# `DTBase` Docs

## Summary

This directory contains the Python package called `dtbase` that is installed if you do `pip install .` from the directory above this one, i.e. after doing that, you should be able to open a python session from anywhere and do `import dtbase`.

There are a few importand subdirectories within this package:

### [core](#dtbase-core)

This contains:
* The database schema (in `structure.py`)
* Various common utilities to interact with the database (in `utils.py`)
* Functions to insert and retrieve data from the database (in `queries.py`, `models.py`, `locations.py`, `sensors.py`)

### [backend](#dtbase-backend)

This is a FastAPI application, providing API endpoints for interacting with the database (via the core functions).
The endpoints are grouped into `location`, `model`, and `sensor`, and within each there is a file `routes.py` defining the methods and URLs.

### [frontend](#dtbase-frontend)

This is a Flask application, providing a basic web interface.   This will send and receive HTTP requests to and from the backend, allowing users to insert sensors, locations etc. to the database, and to view time-series plots or data tables.
The frontend pages are grouped into `locations`, `models`, and `sensors` and within each there is a `routes.py` file and at least one `templates/xyz.html` file defining the URLs and the content to be displayed.  There is also Javascript code in `base/static/javascript` that contains code for making plots and tables.

### [functions](#dtbase-functions)

Contains the required code to use services (ingress or models) in Azure Functions.

### [services](#dtbase-services)

This contains the base Classes for all services, including BaseService, BaseModel and BaseIngress. BaseModel and BaseIngress inherit from the BaseService Class.

### [models](#dtbase-models)

This is where the code for specific models is located. Instructions for how to write custom models can be found [here](models/README.md).

### [ingress](#dtbase-ingress)

This is where code for specific data ingress is located. Data ingress is the act of pulling in data from another source such as an external API or database and inserting into the dtbase database via the backend. Instructions for how to write custom ingress classes can be found [here](ingress/README.md).


## DTBase Backend

The backend is the heart of DTBase. The frontend is just a pretty wrapper for the backend, and all the services are just scripts that call various backend API endpoints. All data is stored in the PostgreSQL database, and the backend is the only recommended way to access that database.

The backend is a web app implemented using FastAPI. It takes in REST requests and returns responses.

### Code structure
* `run.sh`. This is script you call to run the FastAPI app.
* `run_localdb.sh`. Just like `run.sh` except sources a different file of secrets and ensures that a local PostgreSQL server is running.
* `create_app.py`. A tiny script that calls `main.create_app()`.
* `main.py`. The module that defines how the FastAPI app is set up, its settings, endpoints, etc.
* `routers`. The API divides into subsections, such as `/user` for user management and `/sensor` for sensor data. Each of these is implemented in a separate file in `routers`.
* `models.py`. Whenever two files in `routers` use the same Pydantic model for some endpoint, that is kept in `models.py`.
* `database`. Everything related to direct access to the PostgreSQL database.
    * `structure.py`. Defines what are all the tables and their columns and constraints.
    * `locations.py/models.py/services.py` etc. Provide add/edit/delete functions for all the things stored in the database, such as locations, sensors, sensor data, models, model data, and users.
    * `queries.py`. More complex SQL queries and queries used by several files.
    * `utils.py`. Miscellaneous utilities for things like creating new database sessions. Most importantly has too module-level constants, `DB_ENGINE` and `DB_SESSION_MAKER`, that are the one-stop-shop of all other modules whenever a connection to the database is needed.
* `exc.py`. Custom exception classes used by various modules.
* `auth.py`. Everything related to authenticating users. Authentication uses JSON Web Tokens (JWT). See below for how this affects using the API.
* `config.py`. Various modes in which the database can be run, e.g. for development or debugging purposes.

### API documentation

Documentation listing all the API endpoints and their payloads, return values, etc., is automatically generated by FastAPI. If you are developing/running locally, and your backend is running at `http://localhost:5000`, you can find these docs at `http://localhost:5000/docs`. Correspondingly for an Azure deployment it will be something like `https://<your-azure-app-name>_backend.azurewebsites.net/docs`.

#### Authentication

To be able to access any of the API end points you need an authentication token. You can get one from the `/auth/login` endpoint using a username and a password. Once you've obtained a token, you need to add it to header of any other API calls you make as a bearer token. So if `/auth/login` returned
```
{
    "access_token": "abc"
    "refresh_token": "xyz"
}
```
then you would call the other end points with the following in the header of the request:
```
Authorization: Bearer abc
```

If your token expires, you can use the refresh token to get a new for some time still, by calling the `/auth/refresh` end point. This one requires setting you header like above, but using the refresh token (`xyz`) rather than the access token (`abc`).

#### Locations

Locations can be defined using any combination of floating point, integer, or string variables.   These variables, known as _LocationIdentifiers_ must be inserted into the database before an actual _Location_ can be entered.  The set of _LocationIdentifiers_ that is sufficient to define a _Location_ is called a _LocationSchema_.   A _Location_ will therefore have a _LocationSchema_, and one _LocationXYZValue_ for each _LocationIdentifier_ within that schema (where _XYZ_ can be "Float", "Integer" or "String").

#### Sensors

The Sensor data model is as follows.   Every _Sensor_ has a _SensorType_ which in turn specifies the variable(s) it can measure - these are known as _SensorMeasures_.  Each _SensorMeasure_ specifies its datatype (float, int, string, or bool), and these are used to define the type of the corresponding _SensorXYZReadings_.   A _Sensor_ may also have a _SensorLocation_, which specifies a _Location_ as defined above, and a time window (possibly open-ended) when the sensor was at that location.

#### Models

TODO: Write a summary of how storing model data works.

#### Users

TODO: Write a summary of how user management works.

### The Default User

When starting a new deployment of a DTBase-based digital twin one encounters a chicken-and-egg dilemma: To be able to create users, or do anything really, with the backend, one needs to first have a registered user (the `/user/create-user` endpoint requires a valid JWT token like every other endpoint). To way out of this is the default user. If one sets the environment variable `DT_DEFAULT_USER_PASS` and starts the backend, at startup time a user with the "email" `default_user@localhost` is created, with the given password. One can use this to log in and create some proper users. One should then unset the `DT_DEFAULT_USER_PASS` environment variable and restart the backend. This causes the default user to be deleted.

Often it's handy to keep the default user around for development purposes, but one should be careful not to leave it on a live deployment with sensitive data stored.


## DTBase Core

This folder holds all code that is used by more than one part of the DTBase package. The main parts of DTBase, such as frontend, backend, and services, should never import anything from one another. If they share any code, that code should be in core.

Currently the only things here are:
* `exc.py` for some custom exception types we raise.
* `utils.py` for miscellaneous utils, mostly for making calling the backend API endpoints smoother.
* `constants.py` which reads in a large number of environment variables that are considered package-level constants. These include things like the URL for the backend and the password of the default user.

## DTBase Frontend

The DTBase frontend is a web server, implemented using Flask, that serves a graphical user interface for the digital twin.

Anything the frontend can achieve in manipulating the twin it does by calling various backend endpoints. It is essentially a graphical interface for the backend with some basic plotting.

The user interfaces of digital twins tend to be largely bespoke. Hence the DTBase frontend is quite basic, and we expect the user to develop it further to serve their own needs, by for instance implementing data dashboards and visualisations that serve their needs. In developing DTBase, our philosophy has been to lead with the backend, and consider frontend features largely nice-to-have addons. Hence some parts of the frontend lag behind the backend in capabilities, and there are things you can currently do only through the backend.

The frontend is a mixture of Python (Flask), Typescript, CSS, and Jinja HTML templates. In this it differs from the rest of the codebase, which is pure Python. Consequently at the root of `dtbase/frontend` are configuration files for `webpack`, `eslint`, and `prettier`, plus a `package.json` that specifies our Typescript dependencies.

Some notable Typescript dependencies are
* Bootstrap 5 for a grid layout of the pages.
* Datatables for styling our many tables.
* Chart.js for plotting.

### Code structure
* `run.sh`. This is the shell script that starts the web server. It does two things of note:
    * Runs the Typescript compiler, producing Javascript that can be included in the pages sent to the browser, using webpack.
    * Runs the Flask app.
  If the environment variable `FLASK_DEBUG` is set to `1` both of these are run in "watch mode", where they watch for updates to local files and recompile/restart as necessary. This is handy when developing.
* `frontend_app.py`. A simple script that calls `create_app` from `app/__init__.py`.
* `app/__init__.py`. Module for setting up the Flask app with all its settings and such.
* `app/home`, `/locations/`, `/models`, `/sensors`, etc. Each of these has the code for the pages in that subsection for the site. They all have a `routes.py` that defines the Flask routes, and a `templates` folder for the Jinja templates.
* `app/base`. Various resources shared by the different sections of the site. Most notably
    * `templates`. All the base Jinja templates for things like the sidebar, the login page, and error pages.
    * `static`. All the CSS and Typescript. See [dtbase/frontend/app/base/static/README.md](app/base/static/README.md) for more details.
    * `static/node_modules`. A symbolic link from `dtbase/frontend/node_modules`. This way all the Typescript dependencies are under the `static` folder and thus easy to load and import from.
* `exc.py`. Custom exception types used by the frontend.
* `config.py`. Configuration options like debug mode and dev mode for the frontend.
* `user.py`. Everything related authentication on the frontend. We use the `flask-login` plugin to handle logins. Logging in simply means getting a JWT token from the backend, and a user is logged in as long as there's a valid JWT token associated with their session.
* `utils.py`. Miscellaneous Python utilities.

### Our Approach to Typescript and Javascript

The vast majority of client-side code is written in Typescript, and it should be in the `/app/base/static/typescript` (henceforth just `typescript`) folder as `.ts` files. Webpack, which gets run by `run.sh` when starting the frontend webserver, sorts out dependencies and transpiles the Typescript into `.js` files in the `/app/base/static/javascript` folder. There will be one `.js` file for every `.ts` file. The Jinja HTML templates can then include these transpiled Javascript files using `<script>` tags.

The only pure, non-typed Javascript one should ever write should be minimal amounts in `<script>` tags in the Jinja templates. The reason we do this at all is that Flask passes some data to the Jinja templates which needs to be further be passed onto functions we've written in Typescript. The typical usage pattern looks something like this. In the HTML template we have

```jinja-html
{% block javascripts %}
{{ super() }}
<script src="{{ url_for('static', filename='javascript/sensor_list_table.js') }}"></script>
<script>
  const sensors_for_each_type = {{ sensors_for_each_type | tojson | safe }};
  window.addEventListener("DOMContentLoaded", (event) => {
    window.updateTable(sensors_for_each_type);
  });
</script>
{% endblock javascripts %}
```

The first `<script>` tag includes a file transpiled from `sensor_list_table.ts`. The second `<script>` tag includes a small snippet that

1. Reads the data passed to us by Flask into a variable `sensors_for_each_type`.
2. On page load calls the function `window.updateTable` defined in `sensor_list_table.ts` with `sensors_for_each_type`.

`sensor_list_table.ts` looks like this:

```typescript
import { initialiseDataTable } from "./datatables";

export function updateTable(sensors_for_each_type) {
  // blahblah, bunch of things happen here
}

window.updateTable = updateTable;
```

It imports from another module we've written using the ES6 import syntax and defines `updateTable`. It then effectively "exports" this function to be visible in the global scope, and thus usable in the above snippet in the Jinja template, by assigning it to `window.updateTable`. (It also exports it in the ES6 exports sense, so that other Typescript modules can use it.)

When reading the `.ts` files, anything assigned to a field of `window` is to be used in the templates. All imports/exports between the typescript files should use the ES6 syntax. The Typescript files should not ever assume the presence of any global variables.


## DTBase Functions

DTBase supports services as any callable API endpoints that send data to the backend when called. These services do not need to be hosted as part of the same deployment as the database, backend, and frontend of DTBase. However, when implementing your own services, it is often handy to be able to host them as part of the same deployment. For that, we use Azure Functions, Azure's concept for serverless compute that runs on demand and releases resources once it's done running. We recommend reading a bit about Azure Functions before trying to understand this code in detail.

This folder holds all the code necessary for running various services as Azure functions. The code here should be minimal in functionality, and should merely implement the necessary glue bits to have an Azure function e.g. run a model or do ingress. The actual model or ingress code should be in their respective folders.

To test the functions locally, run
```
func start
```

You'll need the Azure command line tools for that. You can install them with
```
brew tap azure/functions
brew install azure-functions-core-tools@4
```

### Examples

Currently there are two example functions in this folder: Arima and weather ingress. You can see how their `__init__.py` files handle interfacing with Azure and base your code on their examples. Any folder under `dtbase/functions` with a `function.json` file will be recognised as an Azure function and included in the Docker container built by the GitHub Action.

## DTBase Ingress

This readme details how to write your own data ingress in DTBase using the OpenWeatherMap as an example.

### BaseIngress

The `BaseIngress` class has all the general purpose tools for interacting with the backend. It inherits from `BaseService`. For a custom data ingress, the user should create their own custom data ingress class inheriting from the `BaseIngress` class. For example:

```
class CustomDataIngress(BaseIngress):
    """
    Custom class inheriting from the BaseIngress class for interacting with the OpenWeatherData API.
    """
```

#### Method: `get_service_data`

The user then needs to write a `get_service_data` method in the `CustomDataIngress`. This method should extract data from a source (could be an online API or from disk or anywhere) and return the data:

```
    def get_service_data():
        api_url = 'example/online/api/endpoint'
        data = request.get(api_url)
        return_value = process_data(data)
        return return_value
```

The structure of the data being returned by the function should be as follows:

```
[(endpoint, payload), (endpoint, payload), etc.]
```

Here each `endpoint` is a string for the name of a DTBase backend endpoint. Each `payload` should be in the specific format required by that endpoint. For more details about the backend endpoints see the [backend README.md](../backend/README.md).

For example, if we would like to insert two different types of sensor readings, then the output of `get_service_data` should look something like this:

```
[
    (
        '/sensor/insert-sensor-readings',
        {
            "measure_name": <name of the first sensor measure>,
            "unique_identifier": <sensor unique identifier>,
            "readings": <list of readings>,
            "timestamps": <list of timestamps>
        },
    ),
    (
        '/sensor/insert-sensor-readings',
        {
            "measure_name": <name of the second sensor measure>,
            "unique_identifier": <sensor unique identifier>,
            "readings": <list of readings>,
            "timestamps": <list of timestamps>
        },
    ),
]
```

#### Calling the Ingress Class

The ingress class can then be called like this:

```
ingresser = CustomDataIngress()
ingresser(
    dt_user_email=blahblah@email.com,
    dt_user_password="this is a very bad password",
)
```
The user credentials can be used to log in to the DTBase backend. Any extra keyword arguments passed when calling `ingresser` are passed onwards to `get_service_data`.

The URL for the DTBase backend with which `BaseIngress` communicates is set by an environment variable called `DT_BACKEND_URL`. So on Linux/Mac you would call your model script as something like
```
DT_BACKEND_URL="http://myownserver.runningdtbase.com" python my_very_own_ingress_script.py
```

Behind the scenes calling `ingresser`
1. Runs the `get_service_data` method to extract data from a source
2. Logs into the backend
3. Loops through the data `get_service_data` returns and posts it to the backend.

### OpenWeatherMap Example

This section will now go through the [ingress_weather](ingress_weather.py) example in detail.

The goal of the weather ingress is to extract data from the OpenWeatherData API and enter it into our database via the backend. There are two different APIs depending on whether the user wants historical data or forecasting.

#### 1. Define Payloads

3 constants are defined at the top of `ingress_weather.py`:

- *`SENSOR_TYPE`:* Define the sensor type as detailed by the `/sensor/insert-sensor-type` API endpoint.
- *`SENSOR_OPENWEATHERMAPHISTORICAL`*: Define the sensor as detailed by the `/sensor/insert-sensor` API endpoint. This sensor is for the historical data API.
- *`SENSOR_OPENWEATHERMAPFORECAST`*: Same as previous sensor but for forecasting.

These constants are dictionaries and define the sensor type and sensor payloads. The payload for entering sensor-readings is built in the `get_service_data` method.

#### 2. OpenWeatherDataIngress

We then write a custom class that inherits from `BaseIngress`. There are a number of `_*` methods that are used to handle different combinations of start and end dates given by the user. A lot of this complexity comes from there being two different APIs for historical and forecast data.

The important method is `get_service_data`. This method takes in `dt_from` and `dt_to` arguments to define when the user wants to extract information from the API. There is then some specific preprocessing to get the exact data we want from the API.

Finally, we return data in this format:

```
sensor_type_output = [("/sensor/insert-sensor-type", SENSOR_TYPE)]

sensor_output = [("/sensor/insert-sensor", sensor_payload)]

sensor_readings_output = [
            ("/sensor/insert-sensor-readings", payload) for payload in measure_payloads
        ]

return sensor_type_output + sensor_output + sensor_readings_output
```

**The `get_service_data` method MUST returns a list of tuples structures as `(endpoint, payload)` for the ingress method to integrate into the rest of DTBase.**

#### 3. Uploading data to database

After writing your own custom `get_service_data` method, we can then send it to the database via the DTBase backend. **The backend must be running. Please check the [developer docs](../../DeveloperDocs.md) for how to run the backend locally**.

This is simply done by calling

```
weather_ingress = OpenWeatherDataIngress()
weather_ingress.ingress_data(dt_from, dt_to)
```

Under the hood the class finds the `get_service_data` method, runs the `get_service_data` method, and then calls the backend API to upload the data to the database. It handles authentication and error handling. This method uses any input arguments required in `get_service_data`.

**Note: It uses environment variables for API keys and authentication so ensure you have the correct variables set.** For instance, for the OpenWeatherDataIngress, the environment variables you'll need to set are `DT_OPENWEATHERMAP_APIKEY` and `DT_OPENWEATHERMAP_LAT` and `DT_OPENWEATHERMAP_LONG` for the latitude and longitude for the weather location. And of course `DT_BACKEND_URL` to specify where the backend is to be found (unless it's `localhost`, which is the default value).

The `example_weather_ingress` function shows how to use this code to ingress weather data.

## DTBase Models

This folder hosts two general purpose timeseries forecasting models, ARIMA and HODMD. They work both as useful additions to many digital twins and as examples for how to implement a model that interfaces with DTBase.

Currently (as of 2024-03-08) this remains work in progress. ARIMA is fully functional, but it has some vestige in its code from a time when it was used for a more particular application, that needs to be cleaned up. HODMD is implemented as a model, but hasn't yet been integrated into DTBase using the `BaseModel` class.

The way to implement your own model is to use the `BaseModel` class as described below. We recommend also reading `dtbase/services/README.md`, since `BaseModel` is just an instance of `BaseService`, described there.

### BaseModel

The `BaseModel` class has all the tools for interacting with the DTBase backend. It inherits from `BaseService`. For a custom model, the user should create their own custom Model Class inheriting from the `BaseModel` class. For example:

```
class CustomModel(BaseModel):
    """
    Custom model inheriting from the BaseModel class.
    """
```

#### Method: `get_service_data`

The user then needs to write a `get_service_data` method in `CustomModel`. This method should run the model return the outputs in a particular format, which `BaseModel` will then submit to the the DTBase backend. This might look something like

```
    def get_service_data(some_data):
        model = get_model()
        some_more_data = get_data()
        predictions = model.predict(some_data, some_more_data)
        return predictions
```

The structure of, `predictions`, i.e. the data being returned by `get_service_data`, should be as follows:

```
[(endpoint, payload), (endpoint, payload), etc.]
```
Here `endpoint` is a string that is the name of a DTBase API endpoint, and `payload` is a dictionary or a list that is the payload that that endpoint expects. For models, the endpoints that likely need to be returned are:

- `/model/insert-model`
- `/model/insert-model-scenario`
- `/model/insert-model-measure`
- `/model/insert-model-run`

Even though calls to endpoints like `insert-model` and `insert-model-measure` only need to be run once, it is safe to make every run of the model call those endpoints. If the model/measure/scenario already exists in the database the backend will just ignore the attempt to write a duplicate, and return a 409 status code, which `BaseModel` handles for you.

#### Calling the Model

The model can then be called like this:

```
cm = CustomModel()
cm(
    some_data=my_favourite_data,
    dt_user_email=blahblah@email.com,
    dt_user_password="this is a very bad password",
)
```

The `dt_user_email` and `dt_user_password` arguments are for user credentials that can be used to log into the DTBase backend. Any other keyword arguments, like in this case `some_data`, are passed onto the `get_service_data` function. Note that these have to be passed as keyword arguments, positional ones won't work.

The URL for the DTBase backend with which `BaseModel` communicates is set by an environment variable called `DT_BACKEND_URL`. So on Linux/Mac you would call your model script as something like
```
DT_BACKEND_URL="http://myownserver.runningdtbase.com" python my_very_own_model_running_script.py
```


## DTBase Services

### What Is a Service?

Services are DTBase's concept for things like scripts to do data ingress and to run models. Anything that one might run either on demand or on a schedule, that would perform some sort of task that writes something into the database.

The way DTBase views a service is a bit abstract but very simple: A service is a URL that DTBase may send a REST request to, possible with a payload. Sending such a request means requesting a service to run. A URL might be for instance `https://myownserver.me/please-run-my-best-model`, and the payload might be the model parameters. The service is then expected to send requests back to the DTBase backend for things like writing model outputs or ingressed data.

Services are quite a new construct in DTBase (as of 2024-03-08), and remain work in progress. They can currently be created and run using both the backend and the frontend. For more documentation see the backend and frontend for more details on how that is implemented. There are three important features we are planning to build but haven't gotten to:
* Scheduling services. Currently the only option is to "run now". One should be able to rather say "run every Wednesday morning using these parameters, and every Saturday morning these other parameters".
* Allow services to return non-trivial values. Currently any data that needs to be written into the database has to be done by the service calling the DTBase backend end points. This will always be a common way for a service to work, because they might take a while to run, but we would also like to support the service returning an HTTP response with the data. Currently we log the HTTP response, but don't do anything else with it.
* Make it easier to send data from the database to a service. Currently all you can send to a service is a fixed set of parameters in the JSON payload. If the service wants any data from the backend, it needs to call an end point such as `/sensor/sensor-readings`. We would rather like to be able to configure the calling of the service so that this data could be part of the request payload.

### How To Write a Service?

To write your own service, you need to write a piece of code that takes in an HTTP request, and in response to that does its thing (ingress, modelling, whatever it is) and sends the output to the relevant DTBase backend endpoint. To make this easier, in `base.py` we have implemented three classes for you to subclass: `BaseService`, `BaseModel`, and `BaseIngress`. The latter two are subclasses of `BaseService`, and they are in fact all funtionally equivalent, the only difference is in the documentation, where one is geared more towards implementing data ingress and the other towards implementing a model. They make this process easier, by handling correct formatting of the DTBase backend request, most importantly involving authentication tokens.

For more details on how to use these classes, refer to the detailed docstrings of the classes and their methods. You can also see an example of how to use `BaseIngress` in `dtbase/ingress/ingress_weather.py`.
